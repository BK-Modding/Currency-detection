> get this model's dataset.rar from here: https://drive.google.com/open?id=1E-aukvBDGcKd-N500AZw-E0bz9f6DL7W

> unzip dataset.rar to get the two data folders - training_set and test_set

> training_set contains the training data - 2000 images per class 
and 3 overall classes = 6000 total

> test_set contains the validation_data - 
1000 images per class and 3 classes overall = 3000 total

-----------------------------------------------

Information about the folders: 

- comp_data contains the sample images from which the datasets are generated.

- temp_backup contains a backup of the dataset folder files just in case things go wrong.


Information about the utility scripts:

- create_dataset_folders.py: Will generate the class folders inside test_set.

- create_uv_dataset.py: Will go through the sample images in comp_data to generate a dataset.
  Does not follow the high priority configuration unlike the scripts of the other two models. All images are given the same priority.

- dellall.py: Will delete all the folders inside training_set and test_set. Can be modified to delete folders in other sets as well.

- delallfiles.py: Will delete all the files in the subfolders of the current directory. Move this into any of the dataset folders and execute it to delete all the files in them but retain
  the folders.

- rename.py: Will rename all the image files inside the current folder. Move this into any of the dataset subfolders/folders and execute it for renaming.

- recursive_file_count.py: Will recursively count the number of files in the training_set and test_set subfolders of the current directory.

Information about the mischellanous scripts:

- remove-contrast-brightness-watermark.py: Random file made to modify certain images.

